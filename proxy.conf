[global]
;;;;;;;;;;;;;;;;;;;;;
;; Server settings ;;
;;;;;;;;;;;;;;;;;;;;;

;; The DNS-address your proxy resides on.
hostname=proxy.example.org

;; The port to listen on for HTTP. This is the port your users connect
;; to. If you are using a reverse proxy, this is the port your HTTPD
;; should proxy to.
http_listen_port=8080

;; The port to listen on for HTTPS. See `http_listen_port`.
https_listen_port=40443

;; The ports to rewrite URLs to for HTTP and HTTPS.
http_port=80
https_port=443

;; The certificate to use for HTTPS connections. This should be
;; readable by the user with which you run SwiperProxy.
https_certificate=proxy.example.org.pem



;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Optimization settings ;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; Number of request handler threads.
threadpool_size=64

;; The maximum post size in bytes. Post sizes exceeding this value will
;; be dropped.
max_post_size=1048576

;; The maximum page size in bytes. Page sizes exceeding this value will
;; be dropped.
max_page_size=5242880

;; GZIP compression level to be used. Keep in mind that using data
;; compression in conjunction with HTTPS is vulnerable to
;; CRIME/BREACH-attacks.
gzip_level = 9

;; Compress responses to clients?
gzip_client_response = no

;; Accept GZIP-compressed responses from servers?
gzip_server_response = yes

;; The read/write timeout in seconds of servers your users try to
;; connect to through your proxy.
upstream_timeout=5

;; The connect timeout in seconds of servers your users try to connect
;; to through your proxy.
upstream_connect_timeout=10

;; The read/write timeout in seconds of your clients.
client_timeout=30

;; Attempt to resolve the reverse DNS of client IP addresses before
;; writing to log?
client_resolve=no



;;;;;;;;;;;;;;;;;;;
;; MISCELLANEOUS ;;
;;;;;;;;;;;;;;;;;;;

;; Location of static HTML files served when users visit your proxy
;; directly.
files_location=/opt/SwiperProxy/swiperproxy/htdocs

;; A comma-separated list of client headers to be filtered.
filter_headers=x-forwarded-for,x-real-ip

;; Inject a faked robots.txt to block crawlers? This will override the
;; robots.txt file of proxied servers your users visit.
block_robots=yes

;; Add an X-Forwarded-For header with the client IP?
use_forwarded_for=no

;; Filename of a blocklist.
block_list=blocklist.txt

;; URL your users will be rewritten to when attempting to browse to a
;; blocked server.
block_target=

;; Location of logfiles.
access_log=/var/log/swiperproxy/access.log
error_log=/var/log/swiperproxy/error.log

[rewrites]
# example.org=example.org
